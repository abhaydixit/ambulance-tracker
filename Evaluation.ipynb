{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVTermProject - Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TzlzxkVBnT40",
        "gNq0HoYypo7U",
        "fx7ou_gLouQs"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKCGgv7EqfU1",
        "outputId": "f67366ec-2107-4b32-e6dc-2fff48062769"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec  5 22:06:16 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p99PtyHZq3fI",
        "outputId": "be33c9a2-9a25-49dc-fe4c-5aa0bdea1a5c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiqPpBfNq7C_"
      },
      "source": [
        "import os, sys, math, time, datetime, random, glob, json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Df4LCRq88N"
      },
      "source": [
        "BASE_PATH = '/content/drive/My Drive/CVTermProject/train/'\n",
        "CLASS_PATH = BASE_PATH + 'config/coco.names'\n",
        "DATA_CONFIG_PATH = BASE_PATH + 'config/coco.data'\n",
        "MODEL_CONFIG_PATH = BASE_PATH + 'config/yolov3.cfg'\n",
        "WEIGHTS_PATH = BASE_PATH + 'checkpoints/new/499.weights'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2tytw6FrDi2"
      },
      "source": [
        "# Uncomment below for the pre-trained YOLO model\n",
        "# MODEL_CONFIG_PATH = BASE_PATH + 'config/Copy of yolov3.cfg'\n",
        "# CLASS_PATH = BASE_PATH + 'config/Copy of coco.names'\n",
        "# WEIGHTS_PATH = BASE_PATH + 'config/yolov3.weights'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H411rp_WmeHM"
      },
      "source": [
        "img_size=416\n",
        "conf_thres=0.8\n",
        "nms_thres=0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ9FBlcukpyU"
      },
      "source": [
        "def read_classes(path):\n",
        "  with open(path, \"r\") as file:\n",
        "    return file.read().strip().split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v1JQxnJkt2w"
      },
      "source": [
        "def read_data(path):\n",
        "  with open(path, \"r\") as file:\n",
        "    data = file.readlines()\n",
        "\n",
        "  config = dict()\n",
        "  for d in data:\n",
        "    d = d.strip()\n",
        "    if len(d) < 1:\n",
        "      continue\n",
        "    key, value = d.split(\"=\")\n",
        "    key = key.strip()\n",
        "    value = value.strip()\n",
        "    config[key] = value\n",
        "  \n",
        "  return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd4l2-09nKXZ"
      },
      "source": [
        "def parse_model_config(path):\n",
        "    file = open(path, 'r')\n",
        "    lines = file.read().split('\\n')\n",
        "    lines = [x for x in lines if x and not x.startswith('#')]\n",
        "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
        "    module_defs = []\n",
        "    for line in lines:\n",
        "        if line.startswith('['): # This marks the start of a new block\n",
        "            module_defs.append({})\n",
        "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
        "            if module_defs[-1]['type'] == 'convolutional':\n",
        "                module_defs[-1]['batch_normalize'] = 0\n",
        "        else:\n",
        "            key, value = line.split(\"=\")\n",
        "            value = value.strip()\n",
        "            module_defs[-1][key.rstrip()] = value.strip()\n",
        "\n",
        "    return module_defs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr8Zv9nTk8Ka"
      },
      "source": [
        "### Define YOLO model config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBU7ELRgky8Q"
      },
      "source": [
        "class EmptyLayer(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(EmptyLayer, self).__init__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HalYvL0Ik_AA"
      },
      "source": [
        "class YOLO(nn.Module):\n",
        "  def __init__(self, anchors, num_classes, dimensions):\n",
        "    super(YOLO, self).__init__()\n",
        "    self.anchors = anchors\n",
        "    self.num_anchors = len(anchors)\n",
        "    self.num_classes = num_classes\n",
        "    self.bbox = 5 + num_classes\n",
        "    self.dimensions = dimensions\n",
        "    self.ignore_thres = 0.5\n",
        "    self.lambda_coord = 1\n",
        "\n",
        "    self.mse_loss = nn.MSELoss()\n",
        "    self.bce_loss = nn.BCELoss()\n",
        "    self.ce_loss = nn.CrossEntropyLoss()\n",
        "  \n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    nB = x.size(0)\n",
        "    grid_size = x.size(2)\n",
        "\n",
        "    stride = self.dimensions / grid_size\n",
        "\n",
        "    FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
        "    LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
        "    ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
        "\n",
        "    prediction = x.view(nB, self.num_anchors, self.bbox, grid_size, grid_size).permute(0, 1, 3, 4, 2).contiguous()\n",
        "\n",
        "    # Get outputs\n",
        "    x = torch.sigmoid(prediction[..., 0])  # Center x\n",
        "    y = torch.sigmoid(prediction[..., 1])  # Center y\n",
        "    w = prediction[..., 2]  # Width\n",
        "    h = prediction[..., 3]  # Height\n",
        "    confidence = torch.sigmoid(prediction[..., 4])  # Conf\n",
        "    clas = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n",
        "    scaled_anchors = FloatTensor([(i / stride, j / stride) for i, j in self.anchors])\n",
        "\n",
        "    predicted_boxes = FloatTensor(prediction[..., :4].shape)\n",
        "    predicted_boxes[..., 0] = x.data + torch.arange(grid_size).repeat(grid_size, 1).view([1, 1, grid_size, grid_size]).type(FloatTensor)\n",
        "    predicted_boxes[..., 1] = y.data + torch.arange(grid_size).repeat(grid_size, 1).t().view([1, 1, grid_size, grid_size]).type(FloatTensor)\n",
        "    predicted_boxes[..., 2] = torch.exp(w.data) * scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
        "    predicted_boxes[..., 3] = torch.exp(h.data) * scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
        "\n",
        "    if targets is not None:\n",
        "      if x.is_cuda:\n",
        "        self.mse_loss = self.mse_loss.cuda()\n",
        "        self.bce_loss = self.bce_loss.cuda()\n",
        "        self.ce_loss = self.ce_loss.cuda()\n",
        "\n",
        "      nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls = build_targets(\n",
        "          pred_boxes = predicted_boxes.cpu().data,\n",
        "          pred_conf = confidence.cpu().data,\n",
        "          pred_cls = clas.cpu().data,\n",
        "          target=targets.cpu().data,\n",
        "          anchors=scaled_anchors.cpu().data,\n",
        "          num_anchors = self.num_anchors,\n",
        "          num_classes = self.num_classes,\n",
        "          grid_size=grid_size,\n",
        "          ignore_thres=self.ignore_thres,\n",
        "          img_dim = self.dimensions\n",
        "      )\n",
        "\n",
        "      proposals = int((confidence > 0.5).sum().item())\n",
        "      recall = float(nCorrect / nGT) if nGT else 1\n",
        "\n",
        "      precision = 0\n",
        "\n",
        "      if proposals > 0:\n",
        "        precision = float(nCorrect / proposals)\n",
        "\n",
        "      mask = Variable(mask.type(ByteTensor))\n",
        "      conf_mask = Variable(conf_mask.type(ByteTensor))\n",
        "\n",
        "      tx = Variable(tx.type(FloatTensor), requires_grad=False)\n",
        "      ty = Variable(ty.type(FloatTensor), requires_grad=False)\n",
        "      tw = Variable(tw.type(FloatTensor), requires_grad=False)\n",
        "      th = Variable(th.type(FloatTensor), requires_grad=False)\n",
        "      tconf = Variable(tconf.type(FloatTensor), requires_grad=False)\n",
        "      tcls = Variable(tcls.type(FloatTensor), requires_grad=False)\n",
        "\n",
        "      conf_mask_true = mask\n",
        "      conf_mask_false = conf_mask - mask\n",
        "\n",
        "      loss_x = self.mse_loss(x[mask], tx[mask])\n",
        "      loss_y = self.mse_loss(y[mask], ty[mask])\n",
        "      loss_w = self.mse_loss(w[mask], tw[mask])\n",
        "      loss_h = self.mse_loss(h[mask], th[mask])\n",
        "\n",
        "\n",
        "      loss_conf = self.bce_loss(confidence[conf_mask_false], tconf[conf_mask_false]) + self.bce_loss(\n",
        "                confidence[conf_mask_true], tconf[conf_mask_true])\n",
        "      \n",
        "      loss_cls = (1 / nB) * self.ce_loss(clas[mask], torch.argmax(tcls[mask], 1))\n",
        "      loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
        "\n",
        "      return (loss, loss_x.item(), loss_y.item(), loss_w.item(), loss_h.item(), loss_conf.item(), loss_cls.item(), recall, precision)\n",
        "    else:\n",
        "      output = torch.cat((predicted_boxes.view(nB, -1, 4) * stride,\n",
        "                         confidence.view(nB, -1, 1),\n",
        "                         clas.view(nB, -1, self.num_classes)),\n",
        "                         -1)\n",
        "      \n",
        "      return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80rOwTMQlBaY"
      },
      "source": [
        "class Darknet(nn.Module):\n",
        "\n",
        "  def __init__(self, config_path, img_size=416):\n",
        "    super(Darknet, self).__init__()\n",
        "    self.modules = parse_model_config(config_path)\n",
        "    self.hyperparameters, self.pipeline = create_pipeline(self.modules)\n",
        "    self.img_size = img_size\n",
        "    self.seen = 0\n",
        "    self.header_info = np.array([0, 0, self.seen, 0])\n",
        "    self.loss_names = [\"x\", \"y\", \"w\", \"h\", \"conf\", \"cls\", \"recall\", \"precision\"]\n",
        "\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    is_training = targets is not None\n",
        "\n",
        "    output = []\n",
        "    self.losses = defaultdict(float)\n",
        "    layer_output = []\n",
        "\n",
        "    for i , (module_def, module) in enumerate(zip(self.modules, self.pipeline)):\n",
        "      if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
        "          x = module(x)\n",
        "      elif module_def[\"type\"] == \"route\":\n",
        "          layer_i = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
        "          x = torch.cat([layer_output[i] for i in layer_i], 1)\n",
        "      elif module_def[\"type\"] == \"shortcut\":\n",
        "          layer_i = int(module_def[\"from\"])\n",
        "          x = layer_output[-1] + layer_output[layer_i]\n",
        "      elif module_def[\"type\"] == \"yolo\":\n",
        "          # Train phase: get loss\n",
        "          if is_training:\n",
        "              x, *losses = module[0](x, targets)\n",
        "              for name, loss in zip(self.loss_names, losses):\n",
        "                  self.losses[name] += loss\n",
        "          # Test phase: Get detections\n",
        "          else:\n",
        "              x = module(x)\n",
        "          output.append(x)\n",
        "      layer_output.append(x)\n",
        "\n",
        "\n",
        "    self.losses[\"recall\"] /= 3\n",
        "    self.losses[\"precision\"] /= 3\n",
        "    return sum(output) if is_training else torch.cat(output, 1)\n",
        "\n",
        "  def load_weights(self, weights_path):\n",
        "    # Open the weights file\n",
        "    fp = open(weights_path, \"rb\")\n",
        "    header = np.fromfile(fp, dtype=np.int32, count=5)  # First five are header values\n",
        "\n",
        "    # Needed to write header when saving weights\n",
        "    self.header_info = header\n",
        "\n",
        "    self.seen = header[3]\n",
        "    weights = np.fromfile(fp, dtype=np.float32)  # The rest are weights\n",
        "    fp.close()\n",
        "\n",
        "    ptr = 0\n",
        "    for i, (module_def, module) in enumerate(zip(self.modules, self.pipeline)):\n",
        "      if module_def[\"type\"] == \"convolutional\":\n",
        "        conv_layer = module[0]\n",
        "        if module_def[\"batch_normalize\"]:\n",
        "            # Load BN bias, weights, running mean and running variance\n",
        "            bn_layer = module[1]\n",
        "            num_b = bn_layer.bias.numel()  # Number of biases\n",
        "            # Bias\n",
        "            bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n",
        "            bn_layer.bias.data.copy_(bn_b)\n",
        "            ptr += num_b\n",
        "            # Weight\n",
        "            bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n",
        "            bn_layer.weight.data.copy_(bn_w)\n",
        "            ptr += num_b\n",
        "            # Running Mean\n",
        "            bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n",
        "            bn_layer.running_mean.data.copy_(bn_rm)\n",
        "            ptr += num_b\n",
        "            # Running Var\n",
        "            bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n",
        "            bn_layer.running_var.data.copy_(bn_rv)\n",
        "            ptr += num_b\n",
        "        else:\n",
        "            # Load conv. bias\n",
        "            num_b = conv_layer.bias.numel()\n",
        "            conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n",
        "            conv_layer.bias.data.copy_(conv_b)\n",
        "            ptr += num_b\n",
        "          # Load conv. weights\n",
        "        num_w = conv_layer.weight.numel()\n",
        "        conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n",
        "        conv_layer.weight.data.copy_(conv_w)\n",
        "        ptr += num_w\n",
        "\n",
        "\n",
        "  def save_weights(self, path, cutoff=-1):\n",
        "\n",
        "    fp = open(path, \"wb\")\n",
        "    self.header_info[3] = self.seen\n",
        "    self.header_info.tofile(fp)\n",
        "\n",
        "    # Iterate through layers\n",
        "    for i, (module_def, module) in enumerate(zip(self.modules[:cutoff], self.pipeline[:cutoff])):\n",
        "        if module_def[\"type\"] == \"convolutional\":\n",
        "            conv_layer = module[0]\n",
        "            # If batch norm, load bn first\n",
        "            if module_def[\"batch_normalize\"]:\n",
        "                bn_layer = module[1]\n",
        "                bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "                bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
        "                bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
        "            # Load conv bias\n",
        "            else:\n",
        "                conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "            # Load conv weights\n",
        "            conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "\n",
        "    fp.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUfzP56-lC2V"
      },
      "source": [
        "def create_pipeline(modules):\n",
        "  hyperparameters = modules.pop(0)\n",
        "  output_filters = [int(hyperparameters['channels'])]\n",
        "  pipeline = nn.ModuleList()\n",
        "\n",
        "  for i, module in enumerate(modules):\n",
        "    mod = nn.Sequential()\n",
        "    module_type = module['type']\n",
        "\n",
        "    if module_type == 'convolutional':\n",
        "      batch_norm = int(module['batch_normalize'])\n",
        "      filters = int(module['filters'])\n",
        "\n",
        "      kernel_size = int(module['size'])\n",
        "\n",
        "      padding = (kernel_size - 1) // 2 if int(module['pad']) else 0\n",
        "\n",
        "      mod.add_module(\"conv_%d\" % i, nn.Conv2d(in_channels = output_filters[-1],\n",
        "                                              out_channels = filters,\n",
        "                                              kernel_size = kernel_size,\n",
        "                                              stride = int(module['stride']),\n",
        "                                              padding = padding,\n",
        "                                              bias = not batch_norm))\n",
        "      \n",
        "      if batch_norm:\n",
        "        mod.add_module('batch_norm_%d' % i, nn.BatchNorm2d(filters))\n",
        "\n",
        "      if module['activation'] == 'leaky':\n",
        "        mod.add_module('leaky_%d' % i, nn.LeakyReLU(0.1))\n",
        "\n",
        "    elif module_type == 'upsample':\n",
        "      stride = int(module['stride'])\n",
        "      mod.add_module('upsample_%d' % i, nn.Upsample(scale_factor=stride, \n",
        "                                                    mode='nearest'))\n",
        "      \n",
        "    elif module_type == 'maxpool':\n",
        "      kernel_size = int(module['size'])\n",
        "      stride = int(module['stride'])\n",
        "\n",
        "      if kernel_size == 2 and stride == 1:\n",
        "        padding = nn.ZeroPad2d((0, 1, 0, 1))\n",
        "        mod.add_module(\"_debug_padding_%d\" % i, padding)\n",
        "\n",
        "      mod.add_module(\"Maxpool_%d\" % i, nn.MaxPool2d(kernel_size=kernel_size,\n",
        "                                                    stride = stride,\n",
        "                                                    padding=int((kernel_size - 1) // 2)))\n",
        "\n",
        "    elif module_type == 'route':\n",
        "      layers = [int(x) for x in module[\"layers\"].split(\",\")]\n",
        "      filters = sum([output_filters[layer_i] for layer_i in layers])\n",
        "      mod.add_module('route_%d' % i, EmptyLayer())\n",
        "\n",
        "    elif module_type == 'shortcut':\n",
        "      filters = output_filters[int(module['from'])]\n",
        "      mod.add_module(\"shortcut_%d\" % i, EmptyLayer())\n",
        "\n",
        "    elif module_type == 'yolo':\n",
        "      anchor_idx = [int(x) for x in module['mask'].split(',')]\n",
        "      anchors = [int(x) for x in module['anchors'].split(',')]\n",
        "\n",
        "      anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
        "      anchors = [anchors[i] for i in anchor_idx]\n",
        "\n",
        "      num_classes = int(module['classes'])\n",
        "      img_height = int(hyperparameters['height'])\n",
        "\n",
        "      mod.add_module('yolo_%d' % i, YOLO(anchors, num_classes, img_height))\n",
        "    pipeline.append(mod)\n",
        "    output_filters.append(filters)\n",
        "\n",
        "  return hyperparameters, pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiuQhoILlHzL"
      },
      "source": [
        "# Load model and weights\n",
        "model = Darknet(MODEL_CONFIG_PATH, img_size=img_size)\n",
        "model.load_weights(WEIGHTS_PATH)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "model.eval()\n",
        "classes = read_classes(CLASS_PATH)\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzlzxkVBnT40"
      },
      "source": [
        "### Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Us7qTXmyVr"
      },
      "source": [
        "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
        "    if not x1y1x2y2:\n",
        "        # Transform from center and width to exact coordinates\n",
        "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
        "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
        "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
        "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
        "    else:\n",
        "        # Get the coordinates of bounding boxes\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
        "\n",
        "    # get the corrdinates of the intersection rectangle\n",
        "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
        "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
        "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
        "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
        "    # Intersection area\n",
        "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
        "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
        "    )\n",
        "    # Union Area\n",
        "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
        "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
        "\n",
        "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "  \n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__sQ58pnmaR"
      },
      "source": [
        "def non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):\n",
        "    box_corner = prediction.new(prediction.shape)\n",
        "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
        "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
        "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
        "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
        "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
        "\n",
        "    output = [None for _ in range(len(prediction))]\n",
        "    image_pred = prediction[0]\n",
        "    conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()\n",
        "    \n",
        "    image_pred = image_pred[conf_mask]\n",
        "\n",
        "    if image_pred.size(0):\n",
        "      class_conf, class_pred = torch.max(image_pred[:, 5 : 5 + num_classes], 1, keepdim=True)\n",
        "      detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n",
        "      unique_labels = detections[:, -1].cpu().unique()\n",
        "      if prediction.is_cuda:\n",
        "          unique_labels = unique_labels.cuda()\n",
        "      for c in unique_labels:\n",
        "          detections_class = detections[detections[:, -1] == c]\n",
        "          _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n",
        "          detections_class = detections_class[conf_sort_index]\n",
        "          max_detections = []\n",
        "          while detections_class.size(0):\n",
        "              max_detections.append(detections_class[0].unsqueeze(0))\n",
        "\n",
        "              if len(detections_class) == 1:\n",
        "                  break\n",
        "              \n",
        "              ious = bbox_iou(max_detections[-1], detections_class[1:])\n",
        "              detections_class = detections_class[1:][ious < nms_thres]\n",
        "\n",
        "          max_detections = torch.cat(max_detections).data\n",
        "          output[0] = (\n",
        "              max_detections if output[0] is None else torch.cat((output[0], max_detections))\n",
        "          )\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vsd2GpWnrMA"
      },
      "source": [
        "def detect_image(img):\n",
        "    # scale and pad image\n",
        "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
        "    imw = round(img.size[0] * ratio)\n",
        "    imh = round(img.size[1] * ratio)\n",
        "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
        "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
        "                        (128,128,128)),\n",
        "         transforms.ToTensor(),\n",
        "         ])\n",
        "    # convert image to Tensor\n",
        "    image_tensor = img_transforms(img).float()\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "    input_img = Variable(image_tensor.type(Tensor))\n",
        "    # run inference on the model and get detections\n",
        "    with torch.no_grad():\n",
        "        detections = model(input_img)\n",
        "        detections = non_max_suppression(detections, 3, conf_thres, nms_thres)\n",
        "        # detections = non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
        "    return detections[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqJA9dqhpD_Z"
      },
      "source": [
        "### Calculate mAP (Mean Average Precision) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWe9kbupBS9d"
      },
      "source": [
        "sum_AP = 0.0\n",
        "ap_dictionary = {}\n",
        "count_true_positives = {}\n",
        "n_classes = len(predicted_bbox)\n",
        "MIN_OVERLAP = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1MDWFjLpTkc"
      },
      "source": [
        "def log_average_miss_rate(prec, rec, num_images):\n",
        "    if prec.size == 0:\n",
        "        lamr = 0\n",
        "        mr = 1\n",
        "        fppi = 0\n",
        "        return lamr, mr, fppi\n",
        "\n",
        "    fppi = (1 - prec)\n",
        "    mr = (1 - rec)\n",
        "\n",
        "    fppi_tmp = np.insert(fppi, 0, -1.0)\n",
        "    mr_tmp = np.insert(mr, 0, 1.0)\n",
        "\n",
        "    ref = np.logspace(-2.0, 0.0, num = 9)\n",
        "    for i, ref_i in enumerate(ref):\n",
        "        j = np.where(fppi_tmp <= ref_i)[-1][-1]\n",
        "        ref[i] = mr_tmp[j]\n",
        "\n",
        "    lamr = math.exp(np.mean(np.log(np.maximum(1e-10, ref))))\n",
        "\n",
        "    return lamr, mr, fppi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSde23uaqGVH"
      },
      "source": [
        "def voc_ap(rec, prec):\n",
        "    rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
        "    rec.append(1.0) # insert 1.0 at end of list\n",
        "    mrec = rec[:]\n",
        "    prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
        "    prec.append(0.0) # insert 0.0 at end of list\n",
        "    mpre = prec[:]\n",
        "    \n",
        "    for i in range(len(mpre)-2, -1, -1):\n",
        "        mpre[i] = max(mpre[i], mpre[i+1])\n",
        "    \n",
        "    i_list = []\n",
        "    for i in range(1, len(mrec)):\n",
        "        if mrec[i] != mrec[i-1]:\n",
        "            i_list.append(i) # if it was matlab would be i + 1\n",
        "    ap = 0.0\n",
        "    for i in i_list:\n",
        "        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
        "    return ap, mrec, mpre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGpOE-JIqgP9"
      },
      "source": [
        "def file_lines_to_list(path):\n",
        "    # open txt file lines to a list\n",
        "    with open(path) as f:\n",
        "        content = f.readlines()\n",
        "    # remove whitespace characters like `\\n` at the end of each line\n",
        "    content = [x.strip() for x in content]\n",
        "    return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ktBJvF0xYa5"
      },
      "source": [
        "def scale_bbox(img_path, x_min, y_min, width, height):\n",
        "  img = Image.open(img_path)\n",
        "  img = np.array(img)\n",
        "  scale_up = img.shape\n",
        "\n",
        "  x_min = x_min * scale_up[1]\n",
        "  y_min = y_min * scale_up[0]\n",
        "\n",
        "  width = width * scale_up[1]\n",
        "  height = height * scale_up[0]\n",
        "\n",
        "  w = width / 2\n",
        "  h = height / 2\n",
        "\n",
        "  x1 = int(x_min - w)\n",
        "  y1 = int(y_min - h)\n",
        "\n",
        "  gt_box = torch.FloatTensor(np.array([x1, y1, width, height])).unsqueeze(0)\n",
        "\n",
        "  return gt_box"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNq0HoYypo7U"
      },
      "source": [
        "### Run below cells to evaluate images from Google open Image Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg_ewI9ApAWy"
      },
      "source": [
        "GT_PATH = BASE_PATH + 'data/dataset/labels'\n",
        "TEMP_FILES_PATH = BASE_PATH + 'dataset/tempFiles'\n",
        "os.makedirs(TEMP_FILES_PATH, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-fmhK2Qqv3U"
      },
      "source": [
        "ground_truth_files_list = glob.glob(GT_PATH + '/*.txt')\n",
        "if len(ground_truth_files_list) == 0:\n",
        "    error(\"Error: No ground-truth files found!\")\n",
        "\n",
        "ground_truth_files_list.sort()\n",
        "gt_counter_per_class = defaultdict(int)\n",
        "counter_images_per_class = defaultdict(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6084jyFrWE7"
      },
      "source": [
        "gt_files = []\n",
        "\n",
        "for txt_file in validation_files:\n",
        "      if len(txt_file) < 1:\n",
        "        continue\n",
        "      txt_file = txt_file.strip()\n",
        "      img_shape = np.array(Image.open(txt_file)).shape\n",
        "      h = img_shape[0]\n",
        "      w = img_shape[1]\n",
        "      txt_file = txt_file.replace('.jpg', '.txt').replace('images', 'labels')\n",
        "      file_id = txt_file.split(\".txt\", 1)[0]\n",
        "      file_id = os.path.basename(os.path.normpath(file_id))\n",
        "      \n",
        "      lines_list = file_lines_to_list(txt_file)\n",
        "      \n",
        "      bounding_boxes = []\n",
        "      \n",
        "      already_seen_classes = []\n",
        "      for line in lines_list:\n",
        "        class_name, left, top, right, bottom = line.split()\n",
        "        bbox = left + ' ' + top + ' ' + right + ' ' + bottom\n",
        "\n",
        "        if class_name == '0':\n",
        "          class_name = 'ambulance'\n",
        "        elif class_name == '1':\n",
        "          class_name = 'building'\n",
        "        else:\n",
        "          class_name = 'person'\n",
        "        \n",
        "        bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False})\n",
        "        gt_counter_per_class[class_name] += 1\n",
        "\n",
        "        if class_name not in already_seen_classes:\n",
        "            counter_images_per_class[class_name] += 1\n",
        "            already_seen_classes.append(class_name)\n",
        "\n",
        "      # dump bounding_boxes into a \".json\" file\n",
        "      new_temp_file = TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n",
        "      gt_files.append(new_temp_file)\n",
        "      with open(new_temp_file, 'w') as outfile:\n",
        "          json.dump(bounding_boxes, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHv6RLK-sMaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a392e9-db38-4cb5-844a-dc9bb855b1b4"
      },
      "source": [
        "gt_counter_per_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'ambulance': 36, 'building': 321, 'person': 533})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc3cm35_urF-"
      },
      "source": [
        "sum_AP = 0.0\n",
        "ap_dictionary = {}\n",
        "lamr_dictionary = {}\n",
        "count_true_positives = {}\n",
        "n_classes = len(predicted_bbox)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fthjK3JEu-0o"
      },
      "source": [
        "for class_name in predicted_bbox.keys():\n",
        "    count_true_positives[class_name] = 0\n",
        "    dr_data = predicted_bbox[class_name]\n",
        "    nd = len(dr_data)\n",
        "    tp = [0] * nd \n",
        "    fp = [0] * nd\n",
        "    idxx = 0\n",
        "    for idx, detection in enumerate(dr_data):\n",
        "\n",
        "        im_path = BASE_PATH + 'data/dataset/images/' + detection[\"file_id\"]\n",
        "\n",
        "        file_id = detection[\"file_id\"].replace('.jpg', '')\n",
        "        gt_file = TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n",
        "        ground_truth_data = json.load(open(gt_file))\n",
        "        ovmax = -1\n",
        "        gt_match = -1\n",
        "\n",
        "        bb = [ float(x) for x in detection[\"bbox\"].split() ]\n",
        "        for obj in ground_truth_data:\n",
        "            \n",
        "            if obj[\"class_name\"] == class_name:\n",
        "                gt = obj[\"bbox\"].split()\n",
        "                gtx1 = float(gt[0])\n",
        "                gty1 = float(gt[1])\n",
        "                gtx2 = float(gt[2])\n",
        "                gty2 = float(gt[3])\n",
        "                gt_box = scale_bbox(im_path, gtx1, gty1, gtx2, gty2)\n",
        "                pred_box = torch.FloatTensor(np.array([bb[0], bb[1], bb[2], bb[3]])).unsqueeze(0)\n",
        "                ov = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n",
        "\n",
        "                # print('\\t gt_box:', gt_box)\n",
        "                # print('\\t pred_box:', pred_box)\n",
        "                # print('\\t\\t iou:', ov)\n",
        "                if ov > ovmax:\n",
        "                    ovmax = ov\n",
        "                    gt_match = obj\n",
        "        \n",
        "        if ovmax >= MIN_OVERLAP:\n",
        "            if not bool(gt_match[\"used\"]):\n",
        "                tp[idx] = 1\n",
        "                gt_match[\"used\"] = True\n",
        "                count_true_positives[class_name] += 1\n",
        "                with open(gt_file, 'w') as f:\n",
        "                        f.write(json.dumps(ground_truth_data))\n",
        "            else:\n",
        "                fp[idx] = 1\n",
        "        else:\n",
        "            fp[idx] = 1\n",
        "            \n",
        "    cumsum = 0\n",
        "    for idx, val in enumerate(fp):\n",
        "        fp[idx] += cumsum\n",
        "        cumsum += val\n",
        "    cumsum = 0\n",
        "    for idx, val in enumerate(tp):\n",
        "        tp[idx] += cumsum\n",
        "        cumsum += val\n",
        "    rec = tp[:]\n",
        "    for idx, val in enumerate(tp):\n",
        "        rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n",
        "    prec = tp[:]\n",
        "    for idx, val in enumerate(tp):\n",
        "        prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n",
        "\n",
        "    ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
        "    sum_AP += ap\n",
        "    text = \"{0:.2f}%\".format(ap*100) + \" = \" + class_name + \" AP \" #class_name + \" AP = {0:.2f}%\".format(ap*100)\n",
        "  \n",
        "    rounded_prec = [ '%.2f' % elem for elem in prec ]\n",
        "    rounded_rec = [ '%.2f' % elem for elem in rec ]\n",
        "  \n",
        "    ap_dictionary[class_name] = ap\n",
        "\n",
        "    n_images = counter_images_per_class[class_name]\n",
        "    lamr, mr, fppi = log_average_miss_rate(np.array(prec), np.array(rec), n_images)\n",
        "    lamr_dictionary[class_name] = lamr\n",
        "\n",
        "mAP = sum_AP / n_classes\n",
        "print(\"mAP = {0:.2f}%\".format(mAP*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY2ez0cYwsFy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evM_2frkotSK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx7ou_gLouQs"
      },
      "source": [
        "### Run below cells for evaluation of tracking data from MOT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niShiXl3W_dD"
      },
      "source": [
        "TRACKING_PATH = '/content/drive/MyDrive/CVTermProject/tracking_data'\n",
        "TRACKING_FRAMES = TRACKING_PATH + '/frames'\n",
        "TRACKING_DET = TRACKING_PATH + '/detections/gt.txt'\n",
        "TEMP_FILES_PATH = '/content/drive/MyDrive/CVTermProject/tracking_data/tempFiles'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vis5HqvqW_lJ"
      },
      "source": [
        "frame_path_list = [TRACKING_FRAMES + '/' + f for f in listdir(TRACKING_FRAMES)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA-EOiFDW_rG"
      },
      "source": [
        "with open(TRACKING_DET) as f:\n",
        "  content = f.readlines()\n",
        "  det_list = [x.strip() for x in content]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dj1lnH8W_5O"
      },
      "source": [
        "os.makedirs(TEMP_FILES_PATH, exist_ok=True)\n",
        "gt_files = []\n",
        "gt_counter_per_class = defaultdict(int)\n",
        "\n",
        "\n",
        "i = 0\n",
        "while i < len(det_list):\n",
        "  bounding_boxes = []\n",
        "  class_name = 'person'\n",
        "  line_data = det_list[i].split(',')\n",
        "  if len(line_data[0]) == 1:\n",
        "    file_id = '00000' + line_data[0]\n",
        "  else:\n",
        "    file_id = '0000' + line_data[0]\n",
        "  prev_frame = line_data[0]\n",
        "  img_file = file_id + '.jpg'\n",
        "  bbox = line_data[2] + ' ' + line_data[3] + ' ' + line_data[4] + ' ' + line_data[5]\n",
        "  bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False})\n",
        "  gt_counter_per_class[class_name] += 1\n",
        "\n",
        "  while True:\n",
        "    if i < len(det_list)-1:\n",
        "      temp_line_data = det_list[i+1].split(',')\n",
        "\n",
        "      if prev_frame == temp_line_data[0]:\n",
        "        i += 1\n",
        "        bbox = temp_line_data[2] + ' ' + temp_line_data[3] + ' ' + temp_line_data[4] + ' ' + temp_line_data[5]\n",
        "        bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False})\n",
        "        gt_counter_per_class[class_name] += 1\n",
        "      else:\n",
        "        break\n",
        "    else:\n",
        "      break\n",
        "  \n",
        "  new_temp_file = TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n",
        "  with open(new_temp_file, 'w') as outfile:\n",
        "      json.dump(bounding_boxes, outfile)\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChemNNoCXACv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e387e109-dc5a-4118-f47a-129da358da74"
      },
      "source": [
        "person = []\n",
        "\n",
        "for _, i in enumerate(frame_path_list):\n",
        "  img_path2 = i.strip()\n",
        "  # print(img_path2)\n",
        "  prev_time = time.time()\n",
        "  img = Image.open(img_path2)\n",
        "  # print(img.size)\n",
        "  try:\n",
        "    detections = detect_image(img)\n",
        "\n",
        "    inference_time = datetime.timedelta(seconds=time.time() - prev_time)\n",
        "    # print ('Inference Time: %s' % (inference_time))\n",
        "\n",
        "    # Get bounding-box colors\n",
        "    cmap = plt.get_cmap('tab20b')\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "    img = np.array(img)\n",
        "    # plt.figure()\n",
        "    # fig, ax = plt.subplots(1, figsize=(12,9))\n",
        "    # ax.imshow(img)\n",
        "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "    unpad_h = img_size - pad_y\n",
        "    unpad_w = img_size - pad_x\n",
        "\n",
        "    if detections is not None:\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        n_cls_preds = len(unique_labels)\n",
        "        bbox_colors = random.sample(colors, n_cls_preds)\n",
        "\n",
        "        # browse detections and draw bounding boxes\n",
        "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
        "            box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
        "            box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
        "            y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
        "            x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
        "            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
        "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor='none')\n",
        "            predicted_class = classes[int(cls_pred)]\n",
        "            # ax.add_patch(bbox)\n",
        "            # plt.text(x1, y1, s=predicted_class, color='white', verticalalignment='top',\n",
        "                    # bbox={'color': color, 'pad': 0})\n",
        "  \n",
        "            temp = dict()\n",
        "            temp[\"confidence\"] = str(cls_conf.cpu().numpy())\n",
        "            temp['file_id'] = img_path2[img_path2.rfind('/') + 1:]\n",
        "            temp['bbox'] = str(x1.cpu().numpy()) + \" \" + str(y1.cpu().numpy()) + \" \" + str(box_w.cpu().numpy()) + \" \" + str(box_h.cpu().numpy())\n",
        "\n",
        "            if predicted_class == 'person':\n",
        "              person.append(temp)\n",
        "              \n",
        "            # print('~~~~   ',predicted_class, y1.cpu().numpy(), x1.cpu().numpy(), box_h.cpu().numpy(), box_w.cpu().numpy(), img_path2[img_path2.rfind('/') + 1:])\n",
        "    # plt.axis('off')\n",
        "    # plt.savefig(img_path2.replace('dataset', 'dataset-output'), bbox_inches='tight', pad_inches=0.0)\n",
        "  except Exception as e:\n",
        "    print(\"e.message = \", e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[139.7048, 173.3145, 186.6420, 317.6860,   0.8142,   0.9999,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[248.6139, 177.8600, 290.3010, 293.3979,   0.9982,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "None\n",
            "None\n",
            "None\n",
            "None\n",
            "tensor([[257.2623, 177.6126, 298.7528, 298.9416,   0.9740,   1.0000,   2.0000],\n",
            "        [109.5424, 108.9908, 316.5731, 330.7109,   0.9374,   0.8966,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[238.4552, 179.3752, 284.0501, 308.4072,   0.8977,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[ 19.6851, 164.2348,  73.7221, 313.7124,   0.9975,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[112.1725, 168.0531, 182.1172, 312.4393,   0.9946,   0.9999,   2.0000],\n",
            "        [190.8297, 205.4443, 212.3777, 279.4699,   0.8405,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[  9.6158,  47.7086, 408.4554, 365.2448,   0.9916,   0.9999,   1.0000]],\n",
            "       device='cuda:0')\n",
            "building\n",
            "tensor([[110.5619, 162.1731, 190.9938, 335.5906,   0.9998,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[189.8597, 183.2703, 233.0494, 299.9836,   1.0000,   1.0000,   2.0000],\n",
            "        [ 20.6311, 152.2117,  74.1039, 312.7435,   0.9930,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[  4.7618, 171.2458,  88.7315, 305.6186,   0.9999,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "tensor([[124.7076, 201.5134, 163.0879, 327.9568,   0.9944,   1.0000,   2.0000],\n",
            "        [315.0376, 194.8754, 344.7794, 280.0753,   0.9670,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[311.3146, 185.4268, 348.7504, 283.0049,   0.9728,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[307.2754, 166.4950, 345.4969, 311.2587,   1.0000,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "tensor([[ 21.1346, 157.7111, 102.2515, 324.7510,   0.9779,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[186.9308, 187.2372, 234.5471, 299.0718,   0.9986,   1.0000,   2.0000],\n",
            "        [ 23.1358, 173.5985,  81.1971, 308.6411,   0.9913,   0.9999,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[305.5110, 165.6682, 342.0070, 307.2073,   1.0000,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "tensor([[129.4398, 185.9859, 162.5951, 287.5138,   0.9220,   1.0000,   2.0000],\n",
            "        [277.3181, 166.5150, 340.6652, 324.3538,   0.9135,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[173.2319, 187.1231, 216.2104, 299.3347,   0.9383,   1.0000,   2.0000],\n",
            "        [  7.6283, 156.6151,  79.9222, 316.8442,   0.8625,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[ 90.8404, 162.4271, 156.8993, 329.7081,   0.8870,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "None\n",
            "tensor([[317.9353, 156.7287, 367.1323, 324.4158,   0.9018,   1.0000,   2.0000],\n",
            "        [141.0383, 173.9556, 187.1633, 311.3838,   0.8088,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[216.9018, 173.5050, 263.3291, 295.8421,   1.0000,   1.0000,   2.0000],\n",
            "        [320.1120, 138.0266, 419.9939, 345.4016,   0.9923,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[197.9360, 200.9466, 225.8791, 274.0195,   0.9390,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "tensor([[186.9228, 203.3640, 237.0391, 334.1214,   0.9999,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "tensor([[ -4.2575, 167.9098,  57.8354, 318.6526,   0.9849,   1.0000,   2.0000],\n",
            "        [199.5803, 188.8420, 237.7240, 295.6633,   0.9725,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "None\n",
            "tensor([[  3.3011, 169.9820,  31.9401, 302.4040,   0.9999,   0.9999,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "None\n",
            "tensor([[  3.1423, 166.1831,  71.9709, 306.8690,   0.9998,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[195.2510, 195.7570, 226.5672, 275.2205,   0.9993,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "tensor([[227.9922, 173.4711, 274.2166, 300.1462,   0.9697,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[196.0922, 188.7783, 236.2280, 294.2256,   0.9948,   1.0000,   2.0000],\n",
            "        [  7.9436, 161.1546,  66.4908, 295.8377,   0.9450,   0.9985,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "None\n",
            "tensor([[108.5457, 166.1988, 189.7759, 320.4444,   0.9746,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[269.9321, 257.0785, 301.8950, 340.3800,   0.9949,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[  1.1490, 171.5780,  40.1657, 304.4572,   0.9979,   0.9999,   2.0000],\n",
            "        [276.4388, 128.5318, 393.3618, 335.4942,   0.8339,   1.0000,   2.0000],\n",
            "        [ 71.0387, 178.1199, 116.1995, 320.2745,   0.8251,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "person\n",
            "tensor([[302.6132, 165.8367, 389.4026, 322.9078,   0.9026,   0.9990,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[220.0288, 175.1216, 265.8470, 296.0339,   0.9004,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[326.1468, 163.8318, 382.8506, 326.5924,   0.9954,   0.9999,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[289.0199, 147.3947, 400.1175, 345.7780,   1.0000,   1.0000,   2.0000],\n",
            "        [ 74.7714, 173.9052, 123.5788, 321.9061,   0.9511,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[162.1487, 155.1406, 261.3743, 314.8760,   0.9918,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "tensor([[  6.3329, 173.4097,  42.7093, 307.7953,   0.9998,   1.0000,   2.0000],\n",
            "        [163.6371, 143.7506, 263.3412, 327.8249,   0.9996,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "None\n",
            "tensor([[176.9643, 240.2892, 198.9681, 289.7164,   0.8849,   0.9999,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[  3.8263, 163.0419,  40.8347, 296.6612,   0.9915,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "None\n",
            "tensor([[251.7800, 174.0072, 292.3403, 300.9862,   0.9977,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n",
            "tensor([[162.7833, 184.3435, 241.3939, 344.6978,   0.9990,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[282.6687, 150.1917, 397.5586, 353.0138,   1.0000,   1.0000,   2.0000],\n",
            "        [205.4627, 183.7082, 245.3953, 287.7381,   0.9884,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[294.8268, 150.9509, 391.2868, 342.7436,   0.9802,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "tensor([[195.1049, 191.1503, 231.7132, 290.9428,   1.0000,   1.0000,   2.0000],\n",
            "        [ 13.9831, 161.4543,  72.8937, 312.6501,   0.8046,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[325.9029, 134.8058, 417.1913, 351.3738,   0.9994,   0.9999,   2.0000],\n",
            "        [226.3278, 180.3638, 267.0918, 299.9439,   0.9971,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "tensor([[ 15.1254, 156.7206,  78.6096, 319.2093,   0.9979,   1.0000,   2.0000],\n",
            "        [280.8455, 260.3935, 310.8287, 340.8832,   0.8941,   1.0000,   2.0000],\n",
            "        [190.3518, 192.5157, 230.3403, 295.0113,   0.8869,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "person\n",
            "person\n",
            "tensor([[227.3195, 186.6956, 269.9256, 295.9441,   1.0000,   1.0000,   2.0000]],\n",
            "       device='cuda:0')\n",
            "person\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_mIOak-8vh7"
      },
      "source": [
        "predicted_bbox = {'person': person}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewvu0TpHAPFf",
        "outputId": "63b9bd18-1ed0-4f94-f105-7b676fccf260"
      },
      "source": [
        "for class_name in predicted_bbox.keys():\n",
        "    count_true_positives[class_name] = 0\n",
        "    dr_data = predicted_bbox[class_name]\n",
        "    nd = len(dr_data)\n",
        "    tp = [0] * nd \n",
        "    fp = [0] * nd\n",
        "    idxx = 0\n",
        "    for idx, detection in enumerate(dr_data):\n",
        "\n",
        "        im_path = BASE_PATH + 'data/dataset/images/' + detection[\"file_id\"]\n",
        "\n",
        "        file_id = detection[\"file_id\"].replace('.jpg', '')\n",
        "        gt_file = TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n",
        "        ground_truth_data = json.load(open(gt_file))\n",
        "        ovmax = -1\n",
        "        gt_match = -1\n",
        "\n",
        "        bb = [ float(x) for x in detection[\"bbox\"].split() ]\n",
        "        for obj in ground_truth_data:\n",
        "            \n",
        "            if obj[\"class_name\"] == class_name:\n",
        "                gt = obj[\"bbox\"].split()\n",
        "                gtx1 = float(gt[0])\n",
        "                gty1 = float(gt[1])\n",
        "                gtx2 = float(gt[2])\n",
        "                gty2 = float(gt[3])\n",
        "                # gt_box = scale_bbox(im_path, gtx1, gty1, gtx2, gty2)\n",
        "                gt_box = torch.FloatTensor(np.array([gtx1, gty1, gtx2, gty2])).unsqueeze(0)\n",
        "                pred_box = torch.FloatTensor(np.array([bb[0], bb[1], bb[2], bb[3]])).unsqueeze(0)\n",
        "                ov = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n",
        "                # print('\\t gt_box:', gt_box)\n",
        "                # print('\\t pred_box:', pred_box)\n",
        "                print('\\t\\t iou:', ov)\n",
        "                if ov > ovmax:\n",
        "                    ovmax = ov\n",
        "                    gt_match = obj\n",
        "        \n",
        "        if ovmax >= MIN_OVERLAP:\n",
        "            if not bool(gt_match[\"used\"]):\n",
        "                tp[idx] = 1\n",
        "                gt_match[\"used\"] = True\n",
        "                count_true_positives[class_name] += 1\n",
        "                print('changed')\n",
        "                with open(gt_file, 'w') as f:\n",
        "                        f.write(json.dumps(ground_truth_data))\n",
        "            else:\n",
        "                fp[idx] = 1\n",
        "        else:\n",
        "            fp[idx] = 1\n",
        "            \n",
        "    cumsum = 0\n",
        "    for idx, val in enumerate(fp):\n",
        "        fp[idx] += cumsum\n",
        "        cumsum += val\n",
        "    cumsum = 0\n",
        "    for idx, val in enumerate(tp):\n",
        "        tp[idx] += cumsum\n",
        "        cumsum += val\n",
        "    rec = tp[:]\n",
        "    for idx, val in enumerate(tp):\n",
        "        rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n",
        "    prec = tp[:]\n",
        "    for idx, val in enumerate(tp):\n",
        "        prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n",
        "\n",
        "    ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
        "    sum_AP += ap\n",
        "    text = \"{0:.2f}%\".format(ap*100) + \" = \" + class_name + \" AP \" #class_name + \" AP = {0:.2f}%\".format(ap*100)\n",
        "\n",
        "mAP = sum_AP / n_classes\n",
        "print(\"mAP = {0:.2f}%\".format(mAP*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6717])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3162])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7470])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6937])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4011])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.1797])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4285])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6324])\n",
            "changed\n",
            "\t\t iou: tensor([0.5819])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7197])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5691])\n",
            "\t\t iou: tensor([0.5042])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3609])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4507])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3589])\n",
            "\t\t iou: tensor([0.6368])\n",
            "\t\t iou: tensor([0.0031])\n",
            "\t\t iou: tensor([0.3222])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5871])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.7193])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4325])\n",
            "changed\n",
            "\t\t iou: tensor([0.2954])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5713])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4583])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.1966])\n",
            "\t\t iou: tensor([0.6242])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.2084])\n",
            "\t\t iou: tensor([0.7940])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.5593])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.3668])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5379])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.0314])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5712])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.8506])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6857])\n",
            "changed\n",
            "\t\t iou: tensor([0.5558])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.2261])\n",
            "\t\t iou: tensor([0.0043])\n",
            "\t\t iou: tensor([0.4312])\n",
            "\t\t iou: tensor([0.7584])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.1599])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6256])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.1957])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6997])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.0315])\n",
            "\t\t iou: tensor([0.8166])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3730])\n",
            "changed\n",
            "\t\t iou: tensor([0.7391])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7273])\n",
            "\t\t iou: tensor([0.0426])\n",
            "\t\t iou: tensor([0.1197])\n",
            "\t\t iou: tensor([0.1764])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3650])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7339])\n",
            "changed\n",
            "\t\t iou: tensor([0.6092])\n",
            "\t\t iou: tensor([0.1647])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5557])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4648])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7473])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5408])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3851])\n",
            "\t\t iou: tensor([0.7228])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.0090])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6360])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3076])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7883])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5166])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.8354])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.1571])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4417])\n",
            "\t\t iou: tensor([0.6271])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3551])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.0257])\n",
            "\t\t iou: tensor([0.0321])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7103])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5062])\n",
            "\t\t iou: tensor([0.1291])\n",
            "\t\t iou: tensor([0.0591])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5762])\n",
            "changed\n",
            "\t\t iou: tensor([0.5615])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6170])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4613])\n",
            "changed\n",
            "\t\t iou: tensor([0.8602])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6924])\n",
            "\t\t iou: tensor([0.1453])\n",
            "\t\t iou: tensor([0.0221])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6135])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5042])\n",
            "\t\t iou: tensor([0.0517])\n",
            "\t\t iou: tensor([0.2229])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6627])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3574])\n",
            "\t\t iou: tensor([0.0385])\n",
            "\t\t iou: tensor([0.2157])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.0852])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.3175])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4772])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5334])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7218])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6225])\n",
            "\t\t iou: tensor([0.1370])\n",
            "\t\t iou: tensor([0.0523])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5169])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.8527])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7935])\n",
            "\t\t iou: tensor([0.1719])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5964])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.7106])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.2577])\n",
            "changed\n",
            "\t\t iou: tensor([0.5933])\n",
            "\t\t iou: tensor([0.1589])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7851])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4946])\n",
            "changed\n",
            "\t\t iou: tensor([0.7483])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.4221])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.0407])\n",
            "\t\t iou: tensor([0.0320])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.6216])\n",
            "\t\t iou: tensor([0.])\n",
            "changed\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.7028])\n",
            "\t\t iou: tensor([0.])\n",
            "\t\t iou: tensor([0.5063])\n",
            "changed\n",
            "mAP = 13.53%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}